Feedforward Neural Networks (FNNs)
Feedforward Neural Networks, also known as Multi-Layer Perceptrons (MLPs), are the simplest form of artificial neural networks. They consist of an input layer, one or more hidden layers, and an output layer.
The basic computation in an FNN can be represented as:

y=f(Wx+b)

Where:
y is the output
f is the activation function
W is the weight matrix
x is the input vector
b is the bias vector

Here's a simple implementation of an FNN in Python using NumPy:

import numpy as np

class FeedforwardNN:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros((1, output_size))

    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

# Usage
nn = FeedforwardNN(input_size=2, hidden_size=3, output_size=1)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output = nn.forward(X)
print(output)
