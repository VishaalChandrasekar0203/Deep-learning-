Transformer Models
Transformers are a more recent architecture that has shown remarkable performance in natural language processing tasks. They rely on self-attention mechanisms to process sequential data.
The self-attention mechanism can be represented as:

Attention(Q,K,V)=softmax( d k QK T )V

Where:
Q, K, and V are query, key, and value matrices
d, k is the dimension of the key vectors

Here's a simplified implementation of self-attention:

import numpy as np

def self_attention(Q, K, V):
    d_k = K.shape[-1]
    scores = np.dot(Q, K.T) / np.sqrt(d_k)
    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
    return np.dot(attention_weights, V)

# Usage
seq_length, d_model = 10, 64
Q = np.random.randn(seq_length, d_model)
K = np.random.randn(seq_length, d_model)
V = np.random.randn(seq_length, d_model)
output = self_attention(Q, K, V)
print(output.shape)
